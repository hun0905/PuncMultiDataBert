{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pickle import TRUE\n",
    "from sympy import Segment\n",
    "from torch.nn.functional import embedding\n",
    "from torch.utils import data\n",
    "from torch.utils.data import dataset\n",
    "import torch\n",
    "from sklearn.model_selection import KFold\n",
    "import time\n",
    "import numpy as np\n",
    "from dataset import PuncDataset\n",
    "from dataset import Collate\n",
    "from model import BertPunc\n",
    "from torchvision.transforms import ToTensor\n",
    "from torch.utils.data import  DataLoader,random_split,SubsetRandomSampler\n",
    "from sklearn.metrics import precision_recall_fscore_support as score\n",
    "from torch.optim.lr_scheduler import ExponentialLR\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report, confusion_matrix\n",
    "from pytorch_pretrained_bert import BertTokenizer\n",
    "from pytorch_pretrained_bert.optimization import BertAdam\n",
    "from pytorch_pretrained_bert import BertTokenizer\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "K_fold = False #是否使用k折交叉驗\n",
    "language = 'zh' #選擇語言\n",
    "fine_tuning = True\n",
    "segment_size = 384\n",
    "save_path = '/home/yunghuan/Desktop/PuncBert/model/BertWiki.pth.tar6'\n",
    "punc_path = '/home/yunghuan/NLP_Dataset/Chinese/data_Ch/punc.txt'\n",
    "data_path = '/home/yunghuan/Desktop/NLP_dataset_high/dataCutForBert/formosaTrain.csv'\n",
    "model_path = '/home/yunghuan/Desktop/PuncBert/model/Bert.pth.tar'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = PuncDataset(data_path,punc_path)#指定所用的dataset為PuncDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Train():\n",
    "    def __init__(self,dataset, model, criterion, optimizer,use_cuda,batch_size,epochs,scheduler,punc_path,load_path = 'punc_model',collate_fn = None,is_continue=False,\\\n",
    "                num_worker = 8,batch_size_times = 1,pin_memory = False,k = 10,with_l1= False,with_l2=False,l1_weight = 0,l2_weight=0,K_fold = False):\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        k = 10\n",
    "        self.start = 0 #決定start 的epoch正常是0 但如果是載入model繼續訓練則會是過去所訓練到的epoch\n",
    "        self.epochs = epochs #epochs的上限 最多訓練幾epoch\n",
    "        self.dataset = dataset #資料 傳入的是PuncData的型態\n",
    "        self.epoch = 0 #目前所到的epcoh\n",
    "        self.model = model #所選的model 可能是 TBRNN或bi-lstm\n",
    "        self.criterion = criterion #所用的loss function 這裡是crossentropy\n",
    "        self.optimizer = optimizer #優化器，用以調整參數，這裡用adam\n",
    "        self.use_cuda = use_cuda #是否使用gpu\n",
    "        self.batch_size = batch_size #batch_size大小\n",
    "        self.scheduler = scheduler #學習率 decay 這裡用exp\n",
    "        self.load_path = load_path #如果要繼續訓練model,指定之前的model所在的path\n",
    "        self.collate_fn = collate_fn \n",
    "        self.is_continue = is_continue #重新訓練一個model或是繼續訓練之前的model\n",
    "        self.num_worker = num_worker #Dataloader的參數，正常時不用刻意調整\n",
    "        self.batch_size_times = batch_size_times #batch_size*batch_size_times就是真正的batch_size大小，因為gpu不足batch_size不能太大，要更大的batch_size時調整\n",
    "        self.pin_memory = pin_memory #Dataloader的參數,不用調整\n",
    "        self.with_l1 = with_l1 #是否添加l1正則化\n",
    "        self.with_l2 = with_l2 #是否添加l2正則化\n",
    "        self.l1_weight = l1_weight #l1正則化的weight\n",
    "        self.l2_weight = l2_weight #l2正則化的weight\n",
    "        self.K_fold = K_fold #是否使用k折,如果不用,則epoch 全執行完就跳出\n",
    "        self.k = k #如果用k折,k的折數是多少\n",
    "        with open(punc_path, encoding='utf-8') as file: \n",
    "            self.punc2id = { i + 1 : word.strip()for i, word in enumerate(file) } #建立index對punctuation的字典\n",
    "        self.punc2id[0] = \" \" #沒有標點\n",
    "        self.history_train_loss = [] #存每個epoch的train loss\n",
    "        self.history_val_loss = [] #存每個epoch的 val loss\n",
    "        \n",
    "        if is_continue: #是否是繼續訓練舊model\n",
    "            #載入舊model的狀態和各種參數\n",
    "            package = torch.load(load_path)\n",
    "            self.model = self.model.load_model(load_path).cuda()\n",
    "            for p in self.model.bert.parameters():\n",
    "                p.requires_grad = True\n",
    "            self.optimizer.load_state_dict(package['optim_dict'])\n",
    "            self.scheduler.load_state_dict(package['scheduler'])\n",
    "            self.start = package['epoch']\n",
    "            self.history_train_loss = package['train_loss']\n",
    "            self.history_val_loss = package['val_loss']\n",
    "        torch.manual_seed(42) \n",
    "        self.splits=KFold(n_splits=k,shuffle=True,random_state=42) #將整個train dataset隨機分k份 ,k-1用來train , 一份用來validation\n",
    "    def prfs(self,train_trues,train_preds,total_loss): #計算和評估各種指標並輸出\n",
    "        precision, recall, fscore, support = score(train_trues, train_preds)#將label和predict比較，計算出各類別Precision,Recall,和F-score\n",
    "        accuracy = accuracy_score(train_trues, train_preds) #計算全部的accuracy,包含空白\n",
    "        print(\"Multi-class accuracy: %.2f\" % accuracy) #accuracy 的精確度\n",
    "        SPLIT = \"-\"*(12*4+3) #分隔線\n",
    "        print(SPLIT)#分隔線輸出\n",
    "        #f = lambda x : round(x, 2)\n",
    "        #輸出每個標點符號的各種指標評估結果\n",
    "        for (v, k) in sorted(self.punc2id.items(), key=lambda x:x[1]):\n",
    "            if v >= len(precision): continue\n",
    "            if k == \" \":\n",
    "                k = \"  \"\n",
    "                continue\n",
    "            print(\"Punctuation: {} Precision: {:.3f} Recall: {:.3f} F-Score: {:.3f}\".format(k,precision[v],recall[v],fscore[v]))\n",
    "        print(SPLIT)\n",
    "\n",
    "        #計算和印出overall(總和不分類別)的所有指標\n",
    "        sklearn_accuracy = accuracy_score(train_trues, train_preds) \n",
    "        sklearn_precision = precision_score(train_trues, train_preds, average='micro')\n",
    "        sklearn_recall = recall_score(train_trues, train_preds, average='micro')\n",
    "        sklearn_f1 = f1_score(train_trues, train_preds, average='micro')\n",
    "        print(\"[sklearn_metrics] Total Epoch:{} loss:{:.4f} accuracy:{:.4f} precision:{:.4f} recall:{:.4f} f1:{:.4f}\".format(self.epoch+1, \\\n",
    "            total_loss, sklearn_accuracy, sklearn_precision, sklearn_recall, sklearn_f1))\n",
    "    def train_epoch(self,data_loader):\n",
    "        self.model.train()  #確保layers of model 在train mode\n",
    "        total_loss = 0\n",
    "        train_preds = [] #存放model預估的標點\n",
    "        train_trues = [] #存放label的真實標點\n",
    "        for  i,(data) in enumerate(data_loader):\n",
    "            #print(i)\n",
    "            input ,segment, label = data#輸入的資料(文字換成index),句子長度，label(標點的index)\n",
    "            #print(segment)\n",
    "            if  self.use_cuda:\n",
    "                input = input.cuda()\n",
    "                label = label.cuda()\n",
    "                segment = segment.cuda()\n",
    "                input = input.to(self.device)\n",
    "                label = label.to(self.device)\n",
    "                segment = segment.to(self.device)\n",
    "            #print(label)\n",
    "            outputs = self.model(input,segment)#將資料輸入model(調用model的forward),outputs為評估結果\n",
    "            #將outputs和label的dimension轉換，在用crossentropy評估loss\n",
    "            #print(outputs.size())\n",
    "            outputs = outputs.view(-1, outputs.size(-1))\n",
    "            #print(outputs.size())\n",
    "            if self.use_cuda:\n",
    "                outputs = outputs.to(self.device)\n",
    "            label = label.view(-1)\n",
    "            loss = self.criterion(outputs, label)\n",
    "            loss_with_reg = loss#loss_with_reg是有加入正則化的loss，如果沒加就和loss相等\n",
    "            if self.use_cuda:\n",
    "                loss_with_reg = loss_with_reg.to(self.device)\n",
    "            if self.with_l1: #l1正則化\n",
    "                l1 = 0\n",
    "                l1 += sum ( [p.abs().sum() for p in self.model.encoder.parameters()] )\n",
    "                l1 += sum ( [p.abs().sum() for p in self.model.decoder.parameters()] )\n",
    "                l1 += sum ( [p.abs().sum() for p in self.model.projected.parameters()] )\n",
    "                l1_penalty = self.l1_weight *l1\n",
    "                loss_with_reg += l1_penalty\n",
    "            if self.with_l2: #l2正則化\n",
    "                l2 = 1e-3\n",
    "                l2 += sum ( [(p**2).sum() for p in self.model.encoder.parameters()] )\n",
    "                l2 += sum ( [(p**2).sum() for p in self.model.decoder.parameters()] )\n",
    "                l2 += sum ( [(p**2).sum() for p in self.model.projected.parameters()] )\n",
    "                l2_penalty = self.l2_weight *l2\n",
    "                loss_with_reg += l2_penalty\n",
    "            loss_with_reg.backward()#更新梯度\n",
    "            clipping_value = 2 # arbitrary value of your choosing\n",
    "            torch.nn.utils.clip_grad_norm_(self.model.parameters(), clipping_value)\n",
    "            if (i+1) % self.batch_size_times == 0 or (i+1) == len(data_loader):\n",
    "                self.optimizer.step() #計算weight\n",
    "                self.optimizer.zero_grad() #將梯度清空\n",
    "            total_loss += loss.item()\n",
    "            train_outputs = outputs.argmax(dim=1) #outputs原輸出的是四種class的機率分佈,換成最高機率class的index\n",
    "            #print('train: ',train_outputs)\n",
    "            #print('label',label)\n",
    "            train_preds.extend(train_outputs.detach().cpu().numpy())\n",
    "            train_trues.extend(label.detach().cpu().numpy())\n",
    "            \n",
    "        #if self.scheduler.get_last_lr()[0] > 1.5e-4:\n",
    "        self.scheduler.step()#進行learning rate decay\n",
    "        print('train: ','\\n')\n",
    "        self.prfs(train_trues,train_preds,total_loss)#印出這個epoch的train的結果評估\n",
    "        return total_loss/(i+1)\n",
    "    def val_epoch(self,data_loader):\n",
    "        val_loss = 0\n",
    "        self.model.eval()#告訴model不要學新東西\n",
    "\n",
    "        #後面大致跟train epoch差不多\n",
    "\n",
    "        val_preds = []\n",
    "        val_trues = []\n",
    "        for i,(data) in enumerate(data_loader):\n",
    "            input , segment , label = data\n",
    "            if  self.use_cuda:\n",
    "                input = input.cuda()#換成可傳入gpu的型態\n",
    "                label = label.cuda()\n",
    "                segment = segment.cuda()\n",
    "                input = input.to(self.device)\n",
    "                label = label.to(self.device)\n",
    "                segment = segment.to(self.device)\n",
    "            \n",
    "            outputs = self.model(input,segment)\n",
    "            outputs = outputs.view(-1, outputs.size(-1))\n",
    "            if self.use_cuda:\n",
    "                outputs = outputs.to(self.device)\n",
    "            label = label.view(-1)\n",
    "            loss = self.criterion(outputs, label)\n",
    "            val_loss += loss.item()\n",
    "            val_outputs = outputs.argmax(dim=1)\n",
    "\n",
    "            val_preds.extend(val_outputs.detach().cpu().numpy())\n",
    "            val_trues.extend(label.detach().cpu().numpy())\n",
    "        print(\"validation: \",'\\n') \n",
    "        self.prfs(val_trues,val_preds,val_loss) #印出valdation 結果的評估\n",
    "        return val_loss/(i+1)\n",
    "    def train(self):\n",
    "        for fold, (train_idx,val_idx) in enumerate(self.splits.split(np.arange(len(self.dataset)))):\n",
    "            #train_idx 是被選為train data的資料的idx val_idx 是 val_data的資料的idx\n",
    "            train_sampler = SubsetRandomSampler(train_idx)#定義train的取樣方式，決定train要取哪些資料\n",
    "            val_sampler = SubsetRandomSampler(val_idx)#決定val要取哪些資料\n",
    "            train_loader = DataLoader(self.dataset, batch_size=self.batch_size, sampler=train_sampler,collate_fn=self.collate_fn,num_workers=self.num_worker,pin_memory=self.pin_memory)\n",
    "            val_loader = DataLoader(self.dataset, batch_size=self.batch_size, sampler=val_sampler,collate_fn=self.collate_fn,num_workers=self.num_worker,pin_memory=self.pin_memory)\n",
    "            device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "            #有沒有attention 的model檔名開頭不同\n",
    "           \n",
    "            \n",
    "\n",
    "            print(device,'\\n')\n",
    "\n",
    "            #開始進入epoch，每一個epoch 都會經歷train epoch和val epoch\n",
    "            for epoch in range(self.start,self.epochs):\n",
    "                self.epoch = epoch\n",
    "                train_loss=self.train_epoch(train_loader)#回傳train loss\n",
    "                val_loss=self.val_epoch(val_loader)#回傳val loss\n",
    "                print(f\"Epoch:{self.epoch + 1} ; {self.epochs} average Training Loss:{train_loss} ; average Val Loss:{val_loss} \")\n",
    "                self.history_train_loss.append(train_loss)#將train loss 存入list\n",
    "                self.history_val_loss.append(val_loss) #將val loss 存入 list\n",
    "\n",
    "                #在checkpoint 將model儲存起來 要存取model和optimizer的state和epoch,history train及val loss語各種hyperparameter\n",
    "                #詳細部分可看Seq2Seq model 的 serialize\n",
    "                torch.save( self.model.serialize(self.model,self.optimizer,self.scheduler,epoch,self.history_train_loss,self.history_val_loss)\\\n",
    "                             ,model_path+str(self.epoch+1))   \n",
    "\n",
    "            if self.K_fold == False: #如果沒有k fold在這裡就會停止\n",
    "                break\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad) #計算model用了多少參數，可以大約估算model的大小"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''collate_fn 就是將調用 dataset中的getitem 所得到的資料進行拼接以我們要的形式輸出\n",
    "也就是在 for i,(data) in enumerate(data_loader) 當中 data所得到的資料就是拼接後的結果'''\n",
    "collate_fn =Collate()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "'''\n",
    "如果是with attention mechanism機制，則要先定義encoder和decoder在把他們傳入Seq2Seq的model\n",
    "簡單說所要的的就是先使用encoder把我們的資料進行的編碼，把它們編成特定的資料型態，然後將我們所得到的\n",
    "編碼(rnn的 output也就就是最後一層的輸出) 傳是decoder,在decoder會將rnn的output傳入另一個rnn解碼，\n",
    "再將每個時刻的輸出和所有時刻輸出的序列進行attention,得到最後的解，詳細參考model2\n",
    "而如果沒有with attention 則就是直接使用bi-lstm即可\n",
    "'''\n",
    "'''\n",
    "input_size,embedding_size都如之前所定義，而output_size就是最後輸出的維度（標點數量＋1），num_layers是rnn的層術\n",
    ",p 是dropout,pretrained是embedding是否要用pretrained word vector\n",
    "encoder(input_size,embedding_size,hidden_size,output_size,num_layers,p,pre_trained)\n",
    "decoder(hidden_size,output_size,num_layers,p)\n",
    "Seq2Seq(encoder,decoder,hidden_size)\n",
    "bi_LSTM(input_size,embedding_dim,hidden_size,num_layers,output_size,pretrained = True)\n",
    "'''\n",
    "save_path = '/home/yunghuan/Desktop/PuncBert/model/BertWiki.pth.tar6'\n",
    "model =BertPunc(segment_size,4,0.3)\n",
    "for p in model.bert.parameters():\n",
    "    p.requires_grad = True\n",
    "use_cuda = True\n",
    "if use_cuda:\n",
    "    model = model.cuda()\n",
    "print(model)\n",
    "print('parameters_count:',count_parameters(model))\n",
    "criterion = torch.nn.CrossEntropyLoss(ignore_index=4) #決定loss function\n",
    "optimizer = torch.optim.Adam(model.parameters(),lr = 3e-5,weight_decay=0.0)#決定optimizer （更新weight 的方式）\n",
    "scheduler = ExponentialLR(optimizer, gamma=1,verbose = True) # weight decay的方式（非必要）\n",
    "'''\n",
    "Train(dataset, model, criterion, optimizer,use_cuda,batch_size,epochs,scheduler,punc_path,load_path = 'punc_model',collate_fn = None\n",
    ",is_continue=False,num_worker = 8,batch_size_times = 1,pin_memory = False,k = 10,with_l1= False,with_l2=False,l1_weight = 0,l2_weight=0\n",
    "\n",
    "dataset： 資料 傳入的是PuncData的型態\n",
    "model： 所選的model 可能是 TBRNN或bi-lstm\n",
    "criterion： 所用的loss function 這裡是crossentropy\n",
    "use_cuda： 是否使用gpu\n",
    "batch_size： batch_size大小\n",
    "epochs： epochs的上限 最多訓練幾epoch\n",
    "scheduler： 學習率 decay 這裡用exp \n",
    "punc_path : 標點符號字典路徑\n",
    "collate_fn : 選擇的collate_fn方式 此處用維我們自定義的collate_fn詳見dataset\n",
    "is_continue : 重新訓練一個model或是繼續訓練之前的model\n",
    "num_worker : Dataloader的參數，正常時不用刻意調整\n",
    "batch_size_times : batch_size*batch_size_times就是真正的batch_size大小，因為gpu不足batch_size不能太大，要更大的batch_size時調整\n",
    "pin_memory : Dataloader的參數,不用調整\n",
    "k #如果用k折,k的折數是多少\n",
    "with_l1 : 是否添加l1正則化\n",
    "with_l2 : 是否添加l2正則化\n",
    "l1_weight : l1正則化的weight\n",
    "l2_weight : l2正則化的weight\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not fine_tuning:\n",
    "    BERT = Train(dataset,model,criterion,optimizer,use_cuda,1,30,scheduler,punc_path,save_path,collate_fn,True,batch_size_times=8,num_worker=0)\n",
    "    BERT.train()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if fine_tuning:\n",
    "    load_path = '/home/yunghuan/Desktop/PuncBert/model/BertWiki.pth.tar6'\n",
    "    model = model.load_model(load_path).cuda()\n",
    "    use_cuda = True\n",
    "    if use_cuda:\n",
    "        model = model.cuda()\n",
    "    '''\n",
    "    for param in model.parameters():\n",
    "        param.requires_grad = False\n",
    "    \n",
    "    for param in model.linaer.parameters():\n",
    "        param.requires_grad = True\n",
    "    model.linaer = nn.Linear(hidden_dim*2,4) \n",
    "    model = model.cuda()\n",
    "    '''\n",
    "    print(model)\n",
    "    print('parameters_count:',count_parameters(model))\n",
    "    criterion = torch.nn.CrossEntropyLoss(ignore_index=4) #決定loss function\n",
    "    optimizer = torch.optim.Adam(model.parameters(),lr = 3e-5,weight_decay=0)#決定optimizer （更新weight 的方式）\n",
    "    scheduler = ExponentialLR(optimizer, gamma=1,verbose = True) # weight decay的方式（非必要）\n",
    "    data_path = '/home/yunghuan/Desktop/NLP_dataset_high/dataCutForBert/formosaTrain.csv'\n",
    "    model_path = '/home/yunghuan/Desktop/PuncBert/model/model_finetuning.pth.tar'\n",
    "    dataset = PuncDataset(data_path,punc_path)\n",
    "    BERT = Train(dataset,model,criterion,optimizer,use_cuda,1,30,scheduler,punc_path,save_path,collate_fn,False,batch_size_times=8)\n",
    "    BERT.train()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "4721f9f90a043d92980dd1afa43594e599d70ba0a83e7f896b6179dbac8637e7"
  },
  "kernelspec": {
   "display_name": "Python 3.8.11 64-bit ('train': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
